import{_ as i}from"./Post.3c6da465.js";import{q as o,o as r,k as l,w as c,a as e,z as s}from"./vendor.09be1bf3.js";import"./app.3d6c68a9.js";var m="/batching.png",h="/downstream_pressure.png";const p=e("div",{class:"prose prose-sm text-left max-w-3xl mx-auto"},[e("h2",{id:"%F0%9F%93%A4-amazon-simple-queue-service",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#%F0%9F%93%A4-amazon-simple-queue-service","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F4E4} Amazon Simple Queue Service"),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F4E4}",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f4e4.png"}),s(" Amazon Simple Queue Service ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"SQS is a durable, scaleable message queue that can support any volume of messages, and producers can push at any rate, and once the messages are enqueued into a queue, one of more consumers polls the queue to recieve those messages, and can recieve a batch of messages at the same request."),s(" SQS is a durable, scaleable message queue that can support any volume of messages, and producers can push at any rate, and once the messages are enqueued into a queue, one of more consumers polls the queue to recieve those messages, and can recieve a batch of messages at the same request. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"::: success"),s(" ::: success ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F4A1} To make this service easier to use, and work for "),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F4A1}",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f4a1.png"}),s(" To make this service easier to use, and work for ")]),e("em",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Model Match"),s(" Model Match ")])]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," specifically, we have the "),s(" specifically, we have the ")]),e("code",null,"Model Match Message API"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"."),s(" . ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},":::"),s(" ::: ")])]),e("h2",{id:"at-least-once-delivery-and-visibility-timeout",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#at-least-once-delivery-and-visibility-timeout","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"At least once delivery and visibility timeout"),s(" At least once delivery and visibility timeout ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"SQS has "),s(" SQS has ")]),e("em",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"at least once delivery"),s(" at least once delivery ")])]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," semantics, meaning that the message will be recieved at least once. This is implemented through "),s(" semantics, meaning that the message will be recieved at least once. This is implemented through ")]),e("em",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Visability Timeouts"),s(" Visability Timeouts ")])]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},". When a pollers polls a message the message will become invisible for a short period of time. If the consumer process the message, it is deleted, if they fail to process the message, it is not deleted and will be sent back to the queue. Once the "),s(" . When a pollers polls a message the message will become invisible for a short period of time. If the consumer process the message, it is deleted, if they fail to process the message, it is not deleted and will be sent back to the queue. Once the ")]),e("em",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Visability Timeouts"),s(" Visability Timeouts ")])]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," have expired, the message will be recieved again."),s(" have expired, the message will be recieved again. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"::: warning"),s(" ::: warning ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u26A0\uFE0F"),e("img",{class:"twemoji",draggable:"false",alt:"\u26A0\uFE0F",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/26a0.png"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," If you set the Visibility Timeout to shorter than the time it takes for a batch of messages to be processed, you will run the risk of likely processing messages multiple times."),s(" If you set the Visibility Timeout to shorter than the time it takes for a batch of messages to be processed, you will run the risk of likely processing messages multiple times. ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},":::"),s(" ::: ")])]),e("h2",{id:"lambda-poll-based-invocation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#lambda-poll-based-invocation","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Lambda Poll Based Invocation"),s(" Lambda Poll Based Invocation ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The lambda service polls, on our behalf, the sqs queue and if and when there is a message in the queue, only then does it invoke our code for the lambda function to run."),s(" The lambda service polls, on our behalf, the sqs queue and if and when there is a message in the queue, only then does it invoke our code for the lambda function to run. ")])]),e("h2",{id:"sqs-to-lambda%3A-batching",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sqs-to-lambda%3A-batching","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"SQS to Lambda: Batching"),s(" SQS to Lambda: Batching ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The lambda service will poll the queue for messages, and if there are any messages in the queue, it will invoke the lambda function and bring in a match of messages. This can scale automatically. The lambda function will then process the messages and then delete them from the queue."),s(" The lambda service will poll the queue for messages, and if there are any messages in the queue, it will invoke the lambda function and bring in a match of messages. This can scale automatically. The lambda function will then process the messages and then delete them from the queue. ")])]),e("p",null,[e("img",{src:m,alt:"AWS SQS to Lambda: Batching"})]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"::: success"),s(" ::: success ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F4A1} Batching is set at the lambda level, and is extreamly cost effective. This will result is messages being processed faster, and less lambda invokations. Up to 10,000 messages can be processed at a time if we use batching properly."),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F4A1}",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f4a1.png"}),s(" Batching is set at the lambda level, and is extreamly cost effective. This will result is messages being processed faster, and less lambda invokations. Up to 10,000 messages can be processed at a time if we use batching properly. ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},":::"),s(" ::: ")])]),e("h2",{id:"lambda-concurrency",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#lambda-concurrency","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Lambda Concurrency"),s(" Lambda Concurrency ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"With SQS each event is enqueued in a queue where they can be buffered and reate limited. And they have to ability to use batching, and we have the ability to rate limit this consumtion."),s(" With SQS each event is enqueued in a queue where they can be buffered and reate limited. And they have to ability to use batching, and we have the ability to rate limit this consumtion. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"What if we are sending messages too quickly, and we have an issue with a downstream system, like DynamoDB or S3? Imagine we had 300 connections to the downstream system, and we are sending messages too quickly, and we have an issue with one of the connections."),s(" What if we are sending messages too quickly, and we have an issue with a downstream system, like DynamoDB or S3? Imagine we had 300 connections to the downstream system, and we are sending messages too quickly, and we have an issue with one of the connections. ")])]),e("p",null,[e("img",{src:h,alt:"AWS SQS to Lambda: Concurrency"})]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"For these reasons we can rate limit the funciton consumtion. Per function we controll concurrency with "),s(" For these reasons we can rate limit the funciton consumtion. Per function we controll concurrency with ")]),e("code",null,"ReservedConcurrentExecutions"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},". This concurrency ceiling allows us to cap the speed at which we drain the queue."),s(" . This concurrency ceiling allows us to cap the speed at which we drain the queue. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"::: danger"),s(" ::: danger ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u26D4\uFE0F It is possible that we could crash elastic search, or S3, or DynamoDB, or whatever, if we are sending messages too quickly."),e("img",{class:"twemoji",draggable:"false",alt:"\u26D4\uFE0F",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/26d4.png"}),s(" It is possible that we could crash elastic search, or S3, or DynamoDB, or whatever, if we are sending messages too quickly. ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},":::"),s(" ::: ")])]),e("h2",{id:"lambda-poller-scaling",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#lambda-poller-scaling","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Lambda Poller Scaling"),s(" Lambda Poller Scaling ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Lambda runs a poller on our behalf, startin with concurrent batchs of 5 invocations at any moment. But if those invocations are running and there are more messges remain in the queue, than an additional 60 invokations each minuet afterwords; scaling all the way up until we drain the queue or we reach 1,000 max concurrent invokations. Assuming a match batch size of 10, that means that a single function paired with an SQS event source can process 10,000 messages in parrellel."),s(" Lambda runs a poller on our behalf, startin with concurrent batchs of 5 invocations at any moment. But if those invocations are running and there are more messges remain in the queue, than an additional 60 invokations each minuet afterwords; scaling all the way up until we drain the queue or we reach 1,000 max concurrent invokations. Assuming a match batch size of 10, that means that a single function paired with an SQS event source can process 10,000 messages in parrellel. ")])]),e("h2",{id:"errors",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#errors","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"Errors"),s(" Errors ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"If we send a message to the queue and the queue sends a batch of messages to the poller, and the poller spins up a lambda function. Lets say that 4 of the 5 messages are processed successfully, and the 5th message fails. This means that the lambda will stop and all of the messages will be sent back to the queue."),s(" If we send a message to the queue and the queue sends a batch of messages to the poller, and the poller spins up a lambda function. Lets say that 4 of the 5 messages are processed successfully, and the 5th message fails. This means that the lambda will stop and all of the messages will be sent back to the queue. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"If the 1st message fails, the lambda will stop and all the message will be sent back to the queue. This means that either we will be processing messages more than once, or we will possibly never process messages."),s(" If the 1st message fails, the lambda will stop and all the message will be sent back to the queue. This means that either we will be processing messages more than once, or we will possibly never process messages. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"To solve this issue, the "),s(" To solve this issue, the ")]),e("code",null,"Model Match Message API"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," will do a couple things:"),s(" will do a couple things: ")])]),e("ol",null,[e("li",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"It will try processing all messages, swallowing any errors, until all messages are processed, either succesfully, or rejected. We then manually delete all succesfully processed messages before we throw an error, allowing the "),s(" It will try processing all messages, swallowing any errors, until all messages are processed, either succesfully, or rejected. We then manually delete all succesfully processed messages before we throw an error, allowing the ")]),e("em",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"poison message"),s(" poison message ")])]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," to be sent back to the queue."),s(" to be sent back to the queue. ")])]),e("li",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"To ensure that we are not processing messages more than once, we check on each message processing weather or not the queue has already been processed. If it has, we will not process the message."),s(" To ensure that we are not processing messages more than once, we check on each message processing weather or not the queue has already been processed. If it has, we will not process the message. ")])])]),e("h2",{id:"what-if-it-is-ok-for-my-messages-to-be-processed-multiple-times%3F",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#what-if-it-is-ok-for-my-messages-to-be-processed-multiple-times%3F","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F914}"),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F914}",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f914.png"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," What if it is ok for my messages to be processed multiple times?"),s(" What if it is ok for my messages to be processed multiple times? ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"You can generally destructure the API to allow you to do this and anything else you want. The use case for this may be creating new history records, or updating existing ones. This process does not require huge amounts of processing power, and data will not be ruined if processed multiple times."),s(" You can generally destructure the API to allow you to do this and anything else you want. The use case for this may be creating new history records, or updating existing ones. This process does not require huge amounts of processing power, and data will not be ruined if processed multiple times. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The trade off is basically the time and resources it takes to run DyanmoDB looks ups are slowing down your processing, but are providing some benefit."),s(" The trade off is basically the time and resources it takes to run DyanmoDB looks ups are slowing down your processing, but are providing some benefit. ")])]),e("h2",{id:"%F0%9F%99%85%E2%80%8D%E2%99%82%EF%B8%8F-error-handelilng",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#%F0%9F%99%85%E2%80%8D%E2%99%82%EF%B8%8F-error-handelilng","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F645}\u200D\u2642\uFE0F Error Handelilng"),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F645}\u200D\u2642\uFE0F",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f645-200d-2642-fe0f.png"}),s(" Error Handelilng ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The "),s(" The ")]),e("code",null,"Model Match Message API"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," processes messages in a way that it synchronously processes all messages, and if any message fails, it will throw an error after deleting the messages from the queue that succeded. This means that using this process will require almost no extra processing power."),s(" processes messages in a way that it synchronously processes all messages, and if any message fails, it will throw an error after deleting the messages from the queue that succeded. This means that using this process will require almost no extra processing power. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"::: warning"),s(" ::: warning ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u26A0\uFE0F"),e("img",{class:"twemoji",draggable:"false",alt:"\u26A0\uFE0F",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/26a0.png"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," Use the "),s(" Use the ")]),e("code",null,"Model Match Message API"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," for processing message batches in almost all cases."),s(" for processing message batches in almost all cases. ")]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},":::"),s(" ::: ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"If you want to omit the use of Database writes and just process the messages, you are giving up the ability to check errors in Dynamo, meaning that any client side requirment to see errant messages will not be met. However, you can still find errored messages in the dead letter queue and you can go through logs to find specific errors."),s(" If you want to omit the use of Database writes and just process the messages, you are giving up the ability to check errors in Dynamo, meaning that any client side requirment to see errant messages will not be met. However, you can still find errored messages in the dead letter queue and you can go through logs to find specific errors. ")])]),e("h2",{id:"%F0%9F%91%AB-getting-multiple-deliveries",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#%F0%9F%91%AB-getting-multiple-deliveries","aria-hidden":"true"},"#"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"})]),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"\u{1F46B} Getting Multiple Deliveries"),e("img",{class:"twemoji",draggable:"false",alt:"\u{1F46B}",src:"https://twemoji.maxcdn.com/v/13.1.0/72x72/1f46b.png"}),s(" Getting Multiple Deliveries ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The "),s(" The ")]),e("code",null,"Model Match Message API"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," will check to make sure that a message hasn\u2019t already been processed. It is possible, and out of our control, that a message will show up in the poller more than once, even though we are handeling messages errors."),s(" will check to make sure that a message hasn\u2019t already been processed. It is possible, and out of our control, that a message will show up in the poller more than once, even though we are handeling messages errors. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"To solve the trade off here, you can create the DynamoDB look up at only the queue lambda level. So you are not actually initializing a "),s(" To solve the trade off here, you can create the DynamoDB look up at only the queue lambda level. So you are not actually initializing a ")]),e("code",null,"Message Record"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," until you see it the very first time. This reduces time and processing by 50%, if you do not need to be aware of the message in the database."),s(" until you see it the very first time. This reduces time and processing by 50%, if you do not need to be aware of the message in the database. ")])]),e("p",null,[e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"},"The queue lambda will upsert the "),s(" The queue lambda will upsert the ")]),e("code",null,"Message Record"),e("span",{class:"emoji-wrapper"},[e("span",{class:"native-emoji"}," to the database on the first invokation, and upsert it again on any further invokations. Deleting the record if the message processes succesfully."),s(" to the database on the first invokation, and upsert it again on any further invokations. Deleting the record if the message processes succesfully. ")])])],-1),v={setup(u,{expose:t}){const a={title:"What is SQS",duration:"5 min read",content:`\u{1F4E4} Amazon Simple Queue Service
SQS is a durable, scaleable message queue that can support any volume of messages, and producers can push at any rate, and once the messages are enqueued into a queue, one of more consumers polls the queue to recieve those messages, and can recieve a batch of messages at the same request.
::: success
\u{1F4A1} To make this service easier to use, and work for Model Match specifically, we have the Model Match Message API.
:::
At least once delivery and visibility timeout
SQS has at least once delivery semantics, meaning that the message will be recieved at least once. This is implemented through Visability Timeouts. When a pollers polls a message the message will become invisible for a short period of time. If the consumer process the message, it is deleted, if they fail to process the message, it is not deleted and will be sent back to the queue. Once the Visability Timeouts have expired, the message will be recieved again.
::: warning
:warning: If you set the Visibility Timeout to shorter than the time it takes for a batch of messages to be processed, you will run the risk of likely processing messages multiple times.
:::
Lambda Poll Based Invocation
The lambda service polls, on our behalf, the sqs queue and if and when there is a message in the queue, only then does it invoke our code for the lambda function to run.
SQS to Lambda: Batching
The lambda service will poll the queue for messages, and if there are any messages in the queue, it will invoke the lambda function and bring in a match of messages. This can scale automatically. The lambda function will then process the messages and then delete them from the queue.
AWS SQS to Lambda: Batching
::: success
\u{1F4A1} Batching is set at the lambda level, and is extreamly cost effective. This will result is messages being processed faster, and less lambda invokations. Up to 10,000 messages can be processed at a time if we use batching properly.
:::
Lambda Concurrency
With SQS each event is enqueued in a queue where they can be buffered and reate limited. And they have to ability to use batching, and we have the ability to rate limit this consumtion.
What if we are sending messages too quickly, and we have an issue with a downstream system, like DynamoDB or S3? Imagine we had 300 connections to the downstream system, and we are sending messages too quickly, and we have an issue with one of the connections.
AWS SQS to Lambda: Concurrency
For these reasons we can rate limit the funciton consumtion. Per function we controll concurrency with ReservedConcurrentExecutions. This concurrency ceiling allows us to cap the speed at which we drain the queue. 
::: danger
\u26D4\uFE0F It is possible that we could crash elastic search, or S3, or DynamoDB, or whatever, if we are sending messages too quickly.
:::
Lambda Poller Scaling
Lambda runs a poller on our behalf, startin with concurrent batchs of 5 invocations at any moment. But if those invocations are running and there are more messges remain in the queue, than an additional 60 invokations each minuet afterwords; scaling all the way up until we drain the queue or we reach 1,000 max concurrent invokations. Assuming a match batch size of 10, that means that a single function paired with an SQS event source can process 10,000 messages in parrellel. 
Errors
If we send a message to the queue and the queue sends a batch of messages to the poller, and the poller spins up a lambda function. Lets say that 4 of the 5 messages are processed successfully, and the 5th message fails. This means that the lambda will stop and all of the messages will be sent back to the queue.
If the 1st message fails, the lambda will stop and all the message will be sent back to the queue. This means that either we will be processing messages more than once, or we will possibly never process messages. 
To solve this issue, the Model Match Message API will do a couple things:
It will try processing all messages, swallowing any errors, until all messages are processed, either succesfully, or rejected. We then manually delete all succesfully processed messages before we throw an error, allowing the poison message to be sent back to the queue.
To ensure that we are not processing messages more than once, we check on each message processing weather or not the queue has already been processed. If it has, we will not process the message.

:thinking: What if it is ok for my messages to be processed multiple times?
You can generally destructure the API to allow you to do this and anything else you want. The use case for this may be creating new history records, or updating existing ones. This process does not require huge amounts of processing power, and data will not be ruined if processed multiple times.
The trade off is basically the time and resources it takes to run DyanmoDB looks ups are slowing down your processing, but are providing some benefit.
\u{1F645}\u200D\u2642\uFE0F Error Handelilng
The Model Match Message API processes messages in a way that it synchronously processes all messages, and if any message fails, it will throw an error after deleting the messages from the queue that succeded. This means that using this process will require almost no extra processing power.
::: warning
:warning: Use the Model Match Message API for processing message batches in almost all cases.
::: 
If you want to omit the use of Database writes and just process the messages, you are giving up the ability to check errors in Dynamo, meaning that any client side requirment to see errant messages will not be met. However, you can still find errored messages in the dead letter queue and you can go through logs to find specific errors.
\u{1F46B} Getting Multiple Deliveries
The Model Match Message API will check to make sure that a message hasn&amp#39;t already been processed. It is possible, and out of our control, that a message will show up in the poller more than once, even though we are handeling messages errors. 
To solve the trade off here, you can create the DynamoDB look up at only the queue lambda level. So you are not actually initializing a Message Record until you see it the very first time. This reduces time and processing by 50%, if you do not need to be aware of the message in the database.
The queue lambda will upsert the Message Record to the database on the first invokation, and upsert it again on any further invokations. Deleting the record if the message processes succesfully. 
`,meta:[{property:"og:title",content:"What is SQS"}]};return t({frontmatter:a}),o({title:"What is SQS",meta:[{property:"og:title",content:"What is SQS"}]}),(g,w)=>{const n=i;return r(),l(n,{frontmatter:a},{default:c(()=>[p]),_:1})}}};export{v as default};
